"""

# ğŸ§  Next Word Prediction Using LSTM

A deep learning project that predicts **the next word in a sentence** using an **LSTM-based language model** trained on Shakespeareâ€™s _Hamlet_. It implements a complete NLP pipeline â€” from dataset preprocessing and sequence creation to model training, evaluation, and deployment via **Streamlit**.

---

## ğŸš€ Project Features

- LSTM architecture for next-word sequence prediction
- Trained on Shakespeareâ€™s **_Hamlet_**
- End-to-end workflow: _preprocessing â†’ training â†’ evaluation â†’ deployment_
- **Streamlit** web application for real-time word prediction
- **Early stopping** applied to minimize overfitting

---

## ğŸ“Œ Tech Stack

| Component            | Technology                    |
| -------------------- | ----------------------------- |
| Programming Language | Python                        |
| Deep Learning        | TensorFlow / Keras            |
| NLP Tools            | Tokenizer, Embedding, Padding |
| Deployment           | Streamlit                     |
| Dataset              | Shakespeare â€” _Hamlet_        |

---

## ğŸ“‚ Project Structure

ğŸ“ Next-Word-Prediction-LSTM  
â”‚  
â”œâ”€â”€ ğŸ“ data  
â”‚ â””â”€â”€ hamlet.txt  
â”‚  
â”œâ”€â”€ ğŸ“ notebooks  
â”‚ â””â”€â”€ model_training.ipynb  
â”‚  
â”œâ”€â”€ ğŸ“ saved_model  
â”‚  
â”œâ”€â”€ streamlit_app.py  
â”œâ”€â”€ requirements.txt  
â””â”€â”€ README.md

---

## ğŸ”§ How It Works

### ğŸ”¹ 1. Data Preprocessing

- Load and clean raw text
- Tokenize text and convert words to integer indices
- Create inputâ€“output sequences for next-word prediction
- Pad sequences to uniform length for model input

### ğŸ”¹ 2. Model Architecture

- **Embedding layer**
- **Two stacked LSTM layers**
- **Dense + Softmax output layer** to generate probability for the next word

### ğŸ”¹ 3. Training

- Optimizer: `Adam`
- Loss function: `Categorical Cross-Entropy`
- Early stopping monitors validation loss to prevent overfitting

### ğŸ”¹ 4. Evaluation

Assessed using multiple sentence prompts to determine prediction accuracy and linguistic coherence.

---

## ğŸŒ Streamlit Web App

Run locally:

```bash
pip install -r requirements.txt
streamlit run streamlit_app.py
```

Example:

```bash
 Input: To be or not to
 Output: be
```

---

## ğŸ“ˆ Results

The trained model demonstrates strong ability to auto-complete sentences and replicate Shakespeare-style language.

| Input                  | Predicted Next Word |
| ---------------------- | ------------------- |
| "The king shall never" | speak               |
| "My lord I have"       | done                |
| "To be or not to"      | be                  |

---

## ğŸ› ï¸ Setup & Installation

```bash
git clone <your-repo-link>
cd <repo-name>
pip install -r requirements.txt
```

---

## ğŸ“Œ Future Improvements

- Train on larger multi-author corpora
- Use **Bidirectional LSTM** / **Transformers** for improved accuracy
- Predict **multiple future words** instead of just one
- Export model & offer **REST API** deployment

---

## ğŸ¤ Contributing

Contributions are welcome!  
Feel free to open issues or submit pull requests ğŸ’¡

---

## ğŸ“œ License

This project is licensed under the **MIT License**.
